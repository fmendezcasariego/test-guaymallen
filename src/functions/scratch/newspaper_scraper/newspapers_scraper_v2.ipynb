{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "toc_visible": true
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Newspaper Scraper v2 - Modular OOP Architecture\n",
                "\n",
                "This notebook demonstrates the usage of the modular newspaper scraper that supports 4 Mendoza news portals:\n",
                "- Los Andes\n",
                "- Diario UNO\n",
                "- El Sol\n",
                "- MDZ\n",
                "\n",
                "## Features\n",
                "- ✅ Modular OOP architecture with abstract base class\n",
                "- ✅ Portal-specific scrapers with custom XPath selectors\n",
                "- ✅ Duplicate detection by URL\n",
                "- ✅ Comprehensive logging with progress tracking\n",
                "- ✅ Configurable delays (1s between requests, 2s between portals)\n",
                "- ✅ Export to CSV and JSON\n",
                "- ✅ Timestamp field (scraped_at)"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 1. Imports"
            ],
            "metadata": {
                "id": "imports_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Import the scraper module\n",
                "from newspapers_scraper_v2 import (\n",
                "    NewspaperScraperOrchestrator,\n",
                "    LosAndesScraper,\n",
                "    DiarioUnoScraper,\n",
                "    ElSolScraper,\n",
                "    MDZScraper\n",
                ")\n",
                "\n",
                "import pandas as pd\n",
                "import json"
            ],
            "metadata": {
                "id": "imports"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 2. Scrape All Portals\n",
                "\n",
                "The orchestrator will scrape all 4 portals sequentially with proper delays."
            ],
            "metadata": {
                "id": "scrape_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Initialize the orchestrator\n",
                "orchestrator = NewspaperScraperOrchestrator()\n",
                "\n",
                "# Scrape all portals\n",
                "results = orchestrator.scrape_all()"
            ],
            "metadata": {
                "id": "scrape_all"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 3. View Results as DataFrame"
            ],
            "metadata": {
                "id": "dataframe_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Convert to DataFrame\n",
                "df = orchestrator.to_dataframe()\n",
                "\n",
                "# Display basic info\n",
                "print(f\"Total articles scraped: {len(df)}\")\n",
                "print(\"\\nArticles per newspaper:\")\n",
                "print(df['newspaper'].value_counts())\n",
                "\n",
                "# Display the DataFrame\n",
                "df"
            ],
            "metadata": {
                "id": "view_dataframe"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 4. Inspect Sample Articles"
            ],
            "metadata": {
                "id": "sample_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Display first few articles with key fields\n",
                "df[['newspaper', 'headline', 'date', 'author']].head(10)"
            ],
            "metadata": {
                "id": "sample_articles"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# View a full article (change index to see different articles)\n",
                "article_index = 0\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(f\"ARTICLE #{article_index}\")\n",
                "print(\"=\" * 80)\n",
                "print(f\"Newspaper: {df.iloc[article_index]['newspaper']}\")\n",
                "print(f\"URL: {df.iloc[article_index]['url']}\")\n",
                "print(f\"Headline: {df.iloc[article_index]['headline']}\")\n",
                "print(f\"Date: {df.iloc[article_index]['date']}\")\n",
                "print(f\"Author: {df.iloc[article_index]['author']}\")\n",
                "print(f\"\\nSummary:\\n{df.iloc[article_index]['summary']}\")\n",
                "print(f\"\\nBody (first 500 chars):\\n{df.iloc[article_index]['body'][:500]}...\")\n",
                "print(f\"\\nScraped at: {df.iloc[article_index]['scraped_at']}\")"
            ],
            "metadata": {
                "id": "view_full_article"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 5. Export Data"
            ],
            "metadata": {
                "id": "export_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Export to CSV\n",
                "orchestrator.export_csv(\"news_data.csv\")\n",
                "\n",
                "# Export to JSON\n",
                "orchestrator.export_json(\"news_data.json\")\n",
                "\n",
                "print(\"✅ Data exported successfully!\")"
            ],
            "metadata": {
                "id": "export_data"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 6. Advanced: Scrape Individual Portal\n",
                "\n",
                "You can also scrape individual portals if needed."
            ],
            "metadata": {
                "id": "individual_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Example: Scrape only Los Andes\n",
                "los_andes = LosAndesScraper()\n",
                "los_andes_results = los_andes.scrape()\n",
                "\n",
                "print(f\"Scraped {len(los_andes_results)} articles from Los Andes\")\n",
                "\n",
                "# Convert to DataFrame\n",
                "df_los_andes = pd.DataFrame([\n",
                "    {\"url\": url, **data} \n",
                "    for url, data in los_andes_results.items()\n",
                "])\n",
                "\n",
                "df_los_andes[['headline', 'date']].head()"
            ],
            "metadata": {
                "id": "individual_scraper"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 7. Data Analysis Examples"
            ],
            "metadata": {
                "id": "analysis_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Check for missing data\n",
                "print(\"Missing data summary:\")\n",
                "print(df.isnull().sum())\n",
                "\n",
                "# Check for empty strings\n",
                "print(\"\\nEmpty string counts:\")\n",
                "for col in ['headline', 'summary', 'body', 'date', 'author']:\n",
                "    empty_count = (df[col] == '').sum()\n",
                "    print(f\"{col}: {empty_count}\")"
            ],
            "metadata": {
                "id": "data_quality"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Article length statistics\n",
                "df['body_length'] = df['body'].str.len()\n",
                "\n",
                "print(\"Article body length statistics:\")\n",
                "print(df.groupby('newspaper')['body_length'].describe())"
            ],
            "metadata": {
                "id": "length_stats"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 8. Configuration\n",
                "\n",
                "You can view and modify the scraper configuration if needed."
            ],
            "metadata": {
                "id": "config_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from newspapers_scraper_v2 import SCRAPER_CONFIG\n",
                "\n",
                "print(\"Current configuration:\")\n",
                "print(json.dumps(SCRAPER_CONFIG, indent=2))"
            ],
            "metadata": {
                "id": "view_config"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}